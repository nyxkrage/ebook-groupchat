{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ebook processing script courtesy of Prax (@maldv)  \n",
    "This notebook is licensed under CC-BY-NC 4.0 and the original can be found here: https://huggingface.co/datasets/maldv/cyberpunk/blob/main/epub-processing.ipynb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import pandas as pd\n",
    "import shutil\n",
    "from bs4 import BeautifulSoup, NavigableString, Tag\n",
    "import ebooklib\n",
    "from ebooklib import epub\n",
    "import os\n",
    "import re\n",
    "from typing import Generator, List"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_ebook_html(ebook_path: str, try_chapter : bool = False) -> Generator[tuple, None, None]:\n",
    "    \"\"\"\n",
    "    Parses the HTML content of an EPUB file, yielding only text content from each <p> block,\n",
    "    while skipping specific elements with class 'calibre3' but considering valid text that follows.\n",
    "\n",
    "    Parameters:\n",
    "    - ebook_path (str): The path to the EPUB file.\n",
    "    - try_chapter (bool): If True, the first paragraph of each chapter will be used to determine the chapter title.\n",
    "\n",
    "    Returns:\n",
    "    - text_generator (Generator[tuple, None, None]): A generator yielding text content.\n",
    "    \"\"\"\n",
    "    book = epub.read_epub(ebook_path)\n",
    "    basename = os.path.basename(ebook_path)\n",
    "    noext = os.path.splitext(basename)[0]\n",
    "    chapter_idx = 0\n",
    "    paragraph_idx = 0\n",
    "    cumsum_word_count = 0\n",
    "    for item in book.get_items_of_type(ebooklib.ITEM_DOCUMENT):\n",
    "        content = item.get_content().decode('utf-8')\n",
    "        results = list(html_tokenizer(content, try_chapter))\n",
    "        if len(results) == 0:\n",
    "            continue\n",
    "        chapter_idx += 1\n",
    "        for row in results:\n",
    "            if len(row[1]) == 0:\n",
    "                continue\n",
    "            paragraph_idx += 1\n",
    "            word_count = len((row[1]))\n",
    "            cumsum_word_count += word_count\n",
    "            row = [noext, paragraph_idx, chapter_idx] + list(row[:]) + [word_count, cumsum_word_count]\n",
    "            yield tuple(row)\n",
    "\n",
    "def html_tokenizer(html_content: str, try_chapter) -> Generator[tuple, None, None]:\n",
    "    \"\"\"\n",
    "    Generator function to tokenize HTML content, yielding text content from each <p> block.\n",
    "\n",
    "    Parameters:\n",
    "    - html_content (str): The HTML content to be tokenized.\n",
    "    - try_chapter (bool): If True, the first paragraph of each chapter will be used to determine the chapter title.\n",
    "\n",
    "    Yields:\n",
    "    - text_generator (Generator[tuple, None, None]): A generator yielding text content. \n",
    "    \"\"\"\n",
    "    soup = BeautifulSoup(html_content, 'html.parser')\n",
    "    fix_quote = re.compile(r'“|”|»|«')\n",
    "    fix_threedot = re.compile(r'…')\n",
    "    fix_bars = re.compile(r'\\|\\s*\\|')\n",
    "    fix_spaces = re.compile(r'\\s+')\n",
    "\n",
    "    def extract_and_yield_text(element, accumulated_texts: List[str]):\n",
    "        if isinstance(element, NavigableString):\n",
    "            accumulated_texts.append(str(element))\n",
    "        elif isinstance(element, Tag):\n",
    "            if element.name == 'a' and 'calibre3' in element.get('class', []):\n",
    "                # Skip processing the <a class=\"calibre3\"> tag itself, but not its siblings\n",
    "                #print('skipping', element)\n",
    "                return\n",
    "            if element.name == 'span' and 'italic' in element.get('class', []):\n",
    "                # Append italic text directly to the accumulated_texts list without yielding\n",
    "                accumulated_texts.append(element.get_text())\n",
    "            else:\n",
    "                # Recursively process all children, including those following skipped elements\n",
    "                for child in element.children:\n",
    "                    extract_and_yield_text(child, accumulated_texts)\n",
    "\n",
    "    chapter = None\n",
    "    for i, p_tag in enumerate(soup.find_all('p')):\n",
    "        accumulated_texts = []\n",
    "        # if p's class is calibre14, skip it because it's metadata\n",
    "        if 'calibre14' in p_tag.get('class', []):\n",
    "            #print('skipping', i)\n",
    "            #continue\n",
    "            pass\n",
    "        else:\n",
    "            #print('processing', i)\n",
    "            if i == 0 and try_chapter:\n",
    "                # Instead of processing, this contains our chapter and title\n",
    "                markers = []\n",
    "                for span in p_tag.find_all('span', class_='bold'):\n",
    "                    markers.append(span.get_text())\n",
    "\n",
    "                if len(markers) >= 2:\n",
    "                    chapter = ' '.join(markers)\n",
    "                continue\n",
    "                    \n",
    "        extract_and_yield_text(p_tag, accumulated_texts)\n",
    "        # if our text is '| |', skip it\n",
    "        if '| |' in ' '.join(accumulated_texts):\n",
    "            continue\n",
    "        text = ' '.join([text.strip() for text in accumulated_texts if text.strip()])\n",
    "        text = text.replace('\\n', ' ')\n",
    "        text = text.replace(u'\\xa0', u' ')\n",
    "        text = fix_quote.sub(u'\"', text)\n",
    "        text = fix_threedot.sub(u'...', text)\n",
    "        text = fix_bars.sub(u'', text)\n",
    "        text = fix_spaces.sub(u' ', text)\n",
    "        text = text.strip()\n",
    "        if text.find('Oceano') != -1:\n",
    "            continue\n",
    "        # If the first character is a capital letter, then a space, followed by more capital letters, it is likely the beginning of a chapter and needs to have the space removed\n",
    "        if len(text) == 0:\n",
    "            continue\n",
    "        if len(text) < 4 and text.isnumeric():\n",
    "            continue\n",
    "        #elif len(text) > 2 and text[0].isupper() and text[1] == ' ' and text[2].isupper():\n",
    "        #    text = text[0] + text[2:]\n",
    "        yield chapter, text\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "lines = parse_ebook_html(\"./LarsKepler-TheHypnotist.epub\", try_chapter=False)\n",
    "df = pd.DataFrame(lines, columns=['book_name', 'paragraph_ix', 'chapter_ix', 'chapter_title', 'text', 'char_count', 'cumsum_char_count'])\n",
    "for drop in [1,2,3,4,117,116]:\n",
    "    df = df[df['chapter_ix'] != drop]\n",
    "df = df[~df['text'].str.startswith(\"Chapter \")]\n",
    "\n",
    "df[\"text\"] = df['text'].str.replace(\"’\", \"'\").str.replace(\" . . .\", \"...\")\n",
    "\n",
    "shutil.rmtree(\"out\", ignore_errors=True)\n",
    "os.mkdir(\"out\")\n",
    "for idx, group in df.groupby(\"chapter_ix\"):\n",
    "    with open(f\"out/chapter_{idx}.txt\", 'w') as f:\n",
    "        f.write('\\n'.join(group['text']))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "jupyterenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
